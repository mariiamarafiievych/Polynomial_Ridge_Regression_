{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mzUjMCDFVTwM"
   },
   "source": [
    "# Polynomial Ridge Regression\n",
    "\n",
    "Welcome to your third lab! You will build more Polynomial Regression with L2 regularization.\n",
    "\n",
    "You will be predicting temperature by day time.\n",
    "\n",
    "This lab is a little bit more complex than first two. You will implement this model in OOP way.\n",
    "\n",
    "**You will learn to:**\n",
    "- Build the general architecture of a learning algorithm with OOP in mind:\n",
    "    - Helper functions\n",
    "        - Generation of polynomial_features\n",
    "        - Calculation of Mean Squared Error\n",
    "        - L2 regularization\n",
    "    - Main Model Class\n",
    "        - Initializing parameters\n",
    "        - Training\n",
    "        - Prediction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LULF64T6VTwO"
   },
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment.\n",
    "- [math](https://docs.python.org/3/library/math.html) - just math ;)\n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kzfUtoUEVTwP"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MmMjFAFkVTwS"
   },
   "source": [
    "## 2 - Overview of the Problem set ##\n",
    "\n",
    "**Problem Statement**: You are given a dataset  containing:\n",
    "    - a training set of m_train examples\n",
    "    - a test set of m_test examples\n",
    "    - each example is of shape (number of features, 1), in our case (1, 1)\n",
    "\n",
    "Let's get more familiar with the dataset. Load the data by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8xi1q04PVTwT"
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "def load_data():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    data = np.genfromtxt('time_temp_2016.tsv', delimiter='\\t')\n",
    "    \n",
    "    x = data[:, 0]\n",
    "    x = x.reshape((x.shape[0], 1))\n",
    "    y = data[:, 1]\n",
    "    \n",
    "    train_set_x, test_set_x, train_set_y, test_set_y = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    train_set_y = train_set_y.reshape((1, train_set_y.shape[0]))\n",
    "    test_set_y = test_set_y.reshape((1, test_set_y.shape[0]))\n",
    "    \n",
    "    return train_set_x.T, test_set_x.T, train_set_y, test_set_y, x.T\n",
    "\n",
    "train_set_x, test_set_x, train_set_y, test_set_y, full_feature_set_for_plot = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sSxdzsspVTwV"
   },
   "outputs": [],
   "source": [
    "print(train_set_x.shape, train_set_y.shape, test_set_x.shape, test_set_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Un88y-VcVTwX"
   },
   "source": [
    "Many software bugs in machine learning come from having matrix/vector dimensions that don't fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. \n",
    "\n",
    "**Exercise:** Find the values for:\n",
    "    - m_train (number of training examples)\n",
    "    - m_test (number of test examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "SWTf0wbjVTwX"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 2 lines of code)\n",
    "m_train = \n",
    "m_test = \n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "\n",
    "print (\"\\ntrain_set_x shape: \" + str(train_set_x.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_FyhDrZFVTwa"
   },
   "source": [
    "**Expected Output for m_train, m_test**: \n",
    "<table style=\"width:15%\">\n",
    "  <tr>\n",
    "      <td><b>m_train</b></td>\n",
    "    <td> 245 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td><b>m_test</b></td>\n",
    "    <td> 121 </td> \n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Vy7c-vOVTwb"
   },
   "source": [
    "### Data visualization\n",
    "To familiarize oneself with the data obtained, we will plot the `Temperature` as a function of `Day`. Since our `Day` feature was already normalized to 0-1 range, we will multiply it by 366 to restore the correct day of the year. Let's also add different colors to train and test samples to make it fancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "veYXpJcCVTwc"
   },
   "outputs": [],
   "source": [
    "# Color map\n",
    "cmap = plt.get_cmap('viridis')\n",
    "\n",
    "# Plot the results\n",
    "m1 = plt.scatter(366 * train_set_x, train_set_y, color=cmap(0.9), s=10)\n",
    "m2 = plt.scatter(366 * test_set_x, test_set_y, color=cmap(0.5), s=10)\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Temperature in Celcius')\n",
    "plt.legend((m1, m2), (\"Training data\", \"Test data\"), loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XlJmZ8Z7VTwe"
   },
   "source": [
    "## 3 - Polynomial Ridge Regression algorithm\n",
    "**Mathematical expression of the algorithm**:\n",
    "For one example $x^{(i)}$:\n",
    "\n",
    "Main trick of polynomial regression - feature combination under predefined degree.\n",
    "Let's define our $degree = 3$ and $x^{(i)} = (x_{1})$ - so that we have only one feature as in our current dataset for the sake of simplicity(later you will see generalized solution for any amount of features and any degree).\n",
    "\n",
    "So, having $degree = 3$ and $x^{(i)} = (x^{(i)}_{1})$ we transform our feature vector to be:\n",
    "$$x^{(i)} = ((x^{(i)}_{1})^0=1, x^{(i)}_{1}, (x^{(i)}_{1})^2, (x^{(i)}_{1})^3)$$\n",
    "\n",
    "Having more than 1 features would cause combinations with replacements for each feature under each degree in range [0, $degree$].\n",
    "\n",
    "After that step we have regular well known process, but in this lab we will use a little trick with bias variable.\n",
    "You should already be familiar with it.\n",
    "\n",
    "Main idea is to add 1 to each training example on the first position. It gives us an ability to count our bias inside of the weights vector on the first position too. So, now our $x^{(i)}$ will look like this:\n",
    "$$x^{(i)} = (1, (x^{(i)}_{1})^0=1, x^{(i)}_{1}, (x^{(i)}_{1})^2, (x^{(i)}_{1})^3)$$\n",
    "\n",
    "So, predictor function:\n",
    "$$h^{(i)} =  w^T x^{(i)} \\tag{1}$$\n",
    "\n",
    "<b>Ridge</b> a.k.a <b>L2 Regularization</b>.\n",
    "Having really high order polynomial function we can easily overfit on our training set.\n",
    "The main technique to prevent overfitting called regularization.\n",
    "You should already be familiar with theory of l2 regularization. Our cost function will look like this:\n",
    "\n",
    "The cost is then computed by summing squared diff over all training examples:\n",
    "$$J = \\frac{1}{2}\\sum_{i=1}^{m}(h^{(i)} - y^{(i)})^{2} + \\frac{1}{2}\\lambda ||w||^2_2\\tag{2}$$\n",
    "\n",
    "Where $\\lambda$ is regularization term and $||w||_2$ is Euclidean norm.\n",
    "\n",
    "Now, only gradient have to be defined.\n",
    "$$X = (x^{(1)}, x^{(2)}, ..., x^{(m-1)}, x^{(m)})$$\n",
    "$$H = w^T X = (h^{(1)}, h^{(2)}, ..., h^{(m-1)}, h^{(m)})$$\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = X(H-Y)^T + \\lambda w \\tag{3}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sm9OfaUdVTwe"
   },
   "source": [
    "### 3.1 Helper utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BCjQsPSKVTwf"
   },
   "source": [
    "In this exercise, you will learn more about custom implemenation of \n",
    "    - Generation of polynomial_features\n",
    "    - Calculation of Mean Squared Error\n",
    "    - L2 regularization\n",
    "\n",
    "Let's get more detailed look at these functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W4MJCQB8VTwg"
   },
   "source": [
    "We create function polynomial_features to transform our flat input features into features of higher degrees.\n",
    "\n",
    "As example for degree=3 and features $(x_1, x_2, x_3)$ we will get:\n",
    "\n",
    "$$((x_1, x_2, x_3),3) -> (1, x_1, x_2, x_3, x^2_1, x_1 x_2, x_1 x_3, x^2_2, x_2 x_3, x^2_3, x^3_1, x^2_1 x_2, x^2_1 x_3,  x_1 x^2_2, x_1 x_2 x_3, x_1 x^2_3, x^3_2, x^2_2 x_3, x_2 x^2_3, x^3_3)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wEBlvPFPVTwh"
   },
   "outputs": [],
   "source": [
    "def polynomial_features(X, degree):\n",
    "    \n",
    "    from itertools import combinations_with_replacement \n",
    "    # combinations_with_replacement('ABC', 2) --> AA AB AC BB BC CC\n",
    "    \n",
    "    n_features, n_samples = np.shape(X)\n",
    "    \n",
    "    def index_combinations(): ## (1, 2) => [(1),(2),(1,1),(1,2),(2,2)]\n",
    "        combs = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n",
    "        ##comb = [(),((1),(2)),((1,1),(1,2),(2,2))]\n",
    "        flat_combs = [item for sublist in combs for item in sublist]\n",
    "        ##flat_combs = [(1),(2),(1,1),(1,2),(2,2)]\n",
    "        return flat_combs\n",
    "    \n",
    "    combinations = index_combinations()\n",
    "    \n",
    "    n_output_features = len(combinations)\n",
    "    \n",
    "    X_new = np.empty((n_output_features, n_samples))\n",
    "    \n",
    "    for i, index_combs in enumerate(combinations):  \n",
    "        X_new[i, :] = np.prod(X[index_combs, :], axis=0)\n",
    "        ## if index_combs == (1,2,3) =>  X_new[:,i] = X[:,1] * X[:,2] * X[:,3] \n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qv5t6eHwVTwj"
   },
   "source": [
    "We will be using mean squared error to evaluate accuracy of our model:\n",
    "\n",
    "$$\n",
    "MSE=\\frac{1}{m}\\sum_{i=1}^{m}{(h^{(i)} - y^{(i)})^2} \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "kP8lQFc_VTwk"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: mean_squared_error\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\" Returns the mean squared error between y_true and y_pred \n",
    "    \n",
    "    Arguments:\n",
    "    y_true -- array of true values\n",
    "    y_pred -- array of predicted values\n",
    "    \n",
    "    Returns:\n",
    "    mse -- mean squared error\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    mse = \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "skPGXgmaVTwo"
   },
   "outputs": [],
   "source": [
    "print (\"mse = \" + str(mean_squared_error(np.array([1, 2, 3, 4]), np.array([2, 3, 4, 6]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PbTljBKTVTwp"
   },
   "source": [
    "**Expected Output**: \n",
    "<table style=\"width:15%\">\n",
    "    <tr>\n",
    "         <td>\n",
    "             <b>mse</b>\n",
    "         </td>\n",
    "         <td>\n",
    "            1.75\n",
    "         </td>  \n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6dWE9CWVTwq"
   },
   "source": [
    "Ridge regression a.k.a. L2 regularization(we will use alpha instead of lambda in the code because of python lambda keyword)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "luYMVongVTwr"
   },
   "outputs": [],
   "source": [
    "# GRADED CLASS: l2_regularization\n",
    "\n",
    "class l2_regularization():\n",
    "    \"\"\" Regularization for Ridge Regression \"\"\"\n",
    "    def __init__(self, alpha):\n",
    "        \"\"\" Set alpha \"\"\"\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def __call__(self, w):\n",
    "        \"\"\" \n",
    "        Computes l2 regularization term\n",
    "        \n",
    "        Arguments:\n",
    "        w -- weights\n",
    "\n",
    "        Returns:\n",
    "        term -- 1/2 * alpha * norm(w)^2\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        term = \n",
    "        ### END CODE HERE ###        \n",
    "        return term\n",
    "\n",
    "    def grad(self, w):\n",
    "        \"\"\" \n",
    "        Computes derivative of l2 regularization term\n",
    "        \n",
    "        Arguments:\n",
    "        w -- weights\n",
    "\n",
    "        Returns:\n",
    "        vector -- alpha * w\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        derivative = \n",
    "        ### END CODE HERE ###      \n",
    "        \n",
    "        return derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jj9oruDhVTws"
   },
   "outputs": [],
   "source": [
    "l2 = l2_regularization(0.5)\n",
    "print (\"l2 reg. term = \" + str(l2(np.array([1, 2, 3, 4]))))\n",
    "print (\"l2 grad. = \" + str(l2.grad(np.array([1, 2, 3, 4]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fp5H5XiKVTwv"
   },
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:30%\">\n",
    "    <tr>\n",
    "        <td><b>l2 reg. term</b></td>\n",
    "       <td> 7.5 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>l2 grad.</b></td>\n",
    "       <td> [0.5 1.  1.5 2. ] </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itl-fbgaVTwv"
   },
   "source": [
    "### 3.2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8dQSUzNdVTwx"
   },
   "outputs": [],
   "source": [
    "# GRADED CLASS: PolynomialRidgeRegression\n",
    "\n",
    "class PolynomialRidgeRegression(object):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    degree: int\n",
    "        The degree of the polynomial that the independent variable X will be transformed to.\n",
    "    reg_factor: float\n",
    "        The factor that will determine the amount of regularization and feature\n",
    "        shrinkage. \n",
    "    n_iterations: int\n",
    "        The number of training iterations the algorithm will tune the weights for.\n",
    "    learning_rate: float\n",
    "        The step length that will be used when updating the weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, degree, reg_factor, n_iterations=3000, learning_rate=0.01, print_error=False):\n",
    "        self.degree = degree\n",
    "        self.regularization = l2_regularization(alpha=reg_factor)\n",
    "        self.n_iterations = n_iterations\n",
    "        self.learning_rate = learning_rate\n",
    "        self.print_error = print_error\n",
    "    \n",
    "        \n",
    "    def initialize_with_zeros(self, n_features):\n",
    "        \"\"\"\n",
    "        This function creates a vector of zeros of shape (n_features, 1)\n",
    "        \n",
    "        Arguments:\n",
    "        n_features -- amount of features\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        self.w = \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        ### START CODE HERE ### \n",
    "        # Generate polynomial features (≈ 1 line of code)\n",
    "        X = \n",
    "\n",
    "        # Insert constant ones for bias weights (≈ 1 line of code)\n",
    "        X = \n",
    "        \n",
    "        # Create array\n",
    "        self.initialize_with_zeros(n_features=X.shape[0])\n",
    "\n",
    "        # Do gradient descent for n_iterations\n",
    "        for i in range(self.n_iterations):\n",
    "            # Calculate prediction (≈ 1 line of code)\n",
    "            H = \n",
    "\n",
    "            # Gradient of l2 loss w.r.t w (≈ 1 line of code)\n",
    "            grad_w = \n",
    "\n",
    "            # Update the weights (≈ 1 line of code)\n",
    "            self.w = \n",
    "\n",
    "            if self.print_error and i % 1000 == 0:\n",
    "                # Calculate l2 loss (≈ 1 line of code)\n",
    "                mse = \n",
    "                ### END CODE HERE ###\n",
    "                print (\"MSE after iteration %i: %f\" %(i, mse))\n",
    "        \n",
    "            \n",
    "    def predict(self, X):\n",
    "        ### START CODE HERE ### \n",
    "        # Generate polynomial features (≈ 1 line of code)\n",
    "        X = \n",
    "        \n",
    "        # Insert constant ones for bias weights (≈ 1 line of code)\n",
    "        X = \n",
    "        \n",
    "        # Calculate prediction (≈ 1 line of code)\n",
    "        y_pred = \n",
    "        ### END CODE HERE ### \n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfOMOLKmVTwz"
   },
   "source": [
    "## 4 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DMS3RAf3VTwz"
   },
   "source": [
    "First of all, we should define a maximum possible polynomial degree (`poly_degree`),  learning rate (`learning_rate`), a number of iterations (`num_iteration`) and regularization factor (`reg_factor`) for our model. Often `reg_factor` is chosen with help of cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OMp_eKnqVTw0"
   },
   "outputs": [],
   "source": [
    "poly_degree = 15\n",
    "learning_rate = 0.001\n",
    "n_iterations = 10000\n",
    "reg_factor = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3JPW8kqAVTw2"
   },
   "source": [
    "Now we can initialize our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "27_JfRk5VTw3"
   },
   "outputs": [],
   "source": [
    "model = PolynomialRidgeRegression(\n",
    "    degree=poly_degree, \n",
    "    reg_factor=reg_factor,\n",
    "    learning_rate=learning_rate,\n",
    "    n_iterations=n_iterations,\n",
    "    print_error=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u9yHZ5LDVTw4"
   },
   "source": [
    "Let's train our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Kmcc8nSCVTw4"
   },
   "outputs": [],
   "source": [
    "model.fit(train_set_x, train_set_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FcblItOfVTw6"
   },
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:50%\"> \n",
    "    <tr>\n",
    "        <td><b>MSE after iteration 0 </b></td> \n",
    "        <td> 127.482367 </td>\n",
    "    </tr>\n",
    "      <tr>\n",
    "        <td> <center> $\\vdots$ </center> </td> \n",
    "        <td> <center> $\\vdots$ </center> </td> \n",
    "    </tr>  \n",
    "    <tr>\n",
    "        <td><b>MSE after iteration 9000</b></td> \n",
    "        <td> 12.618504 </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Th6FczyBVTw7"
   },
   "source": [
    "## 4 - Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8gSf84zmVTw8"
   },
   "outputs": [],
   "source": [
    "y_predictions = model.predict(test_set_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gb8W2Z-JVTw-"
   },
   "source": [
    "Let's calculate mean squred error(MSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "NRTQXtDKVTw-"
   },
   "outputs": [],
   "source": [
    "mse = mean_squared_error(test_set_y, y_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6eImX7c5VTxA"
   },
   "outputs": [],
   "source": [
    "print (\"Mean squared error on test set: %s (given by reg. factor: %s)\" % (mse, reg_factor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVehJnrsVTxB"
   },
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:30%\"> \n",
    "    <tr>\n",
    "        <td style=\"width:15%\"> <b>MSE</b></td> \n",
    "        <td> 11.01910317263094 </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ubgGmdSkVTxC"
   },
   "source": [
    "## 5 - Results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "s5REMxqxVTxC"
   },
   "outputs": [],
   "source": [
    "# Color map\n",
    "cmap = plt.get_cmap('viridis')\n",
    "\n",
    "# Predict for all points in set\n",
    "y_val = model.predict(full_feature_set_for_plot)\n",
    "\n",
    "# Plot the results\n",
    "m1 = plt.scatter(366 * train_set_x, train_set_y, color=cmap(0.9), s=10)\n",
    "m2 = plt.scatter(366 * test_set_x, test_set_y, color=cmap(0.5), s=10)\n",
    "plt.plot(366 * full_feature_set_for_plot.T, y_val.T, color='black', linewidth=2, label=\"Prediction\")\n",
    "plt.suptitle(\"Polynomial Ridge Regression\")\n",
    "plt.title(\"MSE: %.2f\" % mse, fontsize=10)\n",
    "plt.xlabel('Day')\n",
    "plt.ylabel('Temperature in Celcius')\n",
    "plt.legend((m1, m2), (\"Training data\", \"Test data\"), loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bIlRbQsgxOsk"
   },
   "source": [
    "## 6 - Conclusion\n",
    "As we can see, our model fits well the hypothesis function to the data. Despite having high-degree polynomials, we prevented overfitting by using the **L2 Regularization** - method for penalizing high magnitudes of parameters estimates. \n",
    "\n",
    "#### What's next:\n",
    "1. Try experimenting with the `reg_factor` to see how this affects the model you have built.\n",
    "2. Compare the results you have obtained with the `sklearn.linear_model.Ridge` model.\n",
    "3. Try this model in the wild! Select your favorite dataset [here](https://www.kaggle.com/datasets?sortBy=hottest&group=public&page=1&pageSize=20&size=small&filetype=all&license=all&tagids=13303), play with it, reiterate."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Polynomial_Ridge_Regression.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
